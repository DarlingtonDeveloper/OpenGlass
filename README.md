# OpenGlass ğŸ•¶ï¸

**Real-time AI-powered smart glasses interface**

[![Swift](https://img.shields.io/badge/Swift-5.9-orange.svg)](https://swift.org)
[![iOS](https://img.shields.io/badge/iOS-17.0+-blue.svg)](https://developer.apple.com/ios/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Planning-yellow.svg)]()

OpenGlass connects Meta Ray-Ban smart glasses to **Gemini Live** and **OpenClaw**, turning them into a personal AI companion with eyes, ears, and hands. Stream video and audio in real-time, get intelligent responses, and execute actions across 56+ skills â€” all hands-free. Built as a native Swift iOS app with a flexible mode system for translation, QR scanning, object spotting, and more.

---

## Architecture

```mermaid
graph TD
    A[Meta Ray-Ban Glasses] -->|Video + Audio| B[OpenGlass iOS App]
    A2[iPhone Camera Fallback] -->|Video + Audio| B
    B --> C[Vision Pipeline\n1fps JPEG frames]
    B --> D[Audio Pipeline\n16kHz PCM]
    B --> E[Mode Router]
    C --> F[Gemini Live WebSocket]
    D --> F
    E --> F
    F --> G[Audio Response â†’ Speaker]
    F --> H[Tool Calls â†’ OpenClaw]
    F --> I[Transcript â†’ Screen]
    H --> J[OpenClaw Gateway\n56+ Skills]
```

## Features

| Mode | Description |
|------|-------------|
| ğŸ¤– **Assistant** | General-purpose AI with vision â€” describe scenes, read signs, remember context |
| ğŸŒ **Translator** | Real-time Mandarin â†” English translation (voice + visual text) |
| ğŸ“± **QR Scanner** | Detect and act on QR codes â€” open links, add contacts, trigger skills |
| ğŸ‘ï¸ **Spotter** | Watch for specific objects/events and alert when spotted |
| ğŸ§­ **Navigator** | Contextual navigation â€” read signs, find places, look up routes |
| âš™ï¸ **Custom** | User-defined modes with custom system instructions and tool sets |

## How It Works

1. **Capture** â€” Video frames from glasses (or iPhone camera) + microphone audio
2. **Stream** â€” Frames throttled to 1fps JPEG, audio at 16kHz PCM, both sent to Gemini Live via WebSocket
3. **Process** â€” Gemini analyses vision + audio, generates responses and tool calls
4. **Act** â€” Audio responses play through the speaker; tool calls route to OpenClaw Gateway for execution
5. **Display** â€” Live transcript shown on screen; results fed back into the conversation

## Getting Started

### Prerequisites

- **Xcode 15+** with Swift 5.9
- **iPhone** running iOS 17.0+ (iPhone 12 or later recommended)
- **Gemini API key** from [Google AI Studio](https://aistudio.google.com/apikey)
- **OpenClaw Gateway** running on a Mac on the same LAN
- **Meta Ray-Ban glasses** (optional â€” iPhone camera works as fallback)

### Build

```bash
git clone https://github.com/DarlingtonDeveloper/OpenGlass.git
cd OpenGlass
open OpenGlass.xcodeproj  # (when Xcode project is created)
```

### Configure

1. Add your Gemini API key in Settings â†’ API Key
2. Ensure OpenClaw Gateway is running (`openclaw gateway status`)
3. Connect to the same Wi-Fi network as your Mac
4. (Optional) Pair Meta Ray-Ban glasses via Bluetooth

See [docs/SETUP.md](docs/SETUP.md) for detailed instructions.

## Project Structure

```
OpenGlass/
â”œâ”€â”€ App/                  # App entry point and root view
â”œâ”€â”€ Config/               # Configuration management
â”œâ”€â”€ Gemini/               # Gemini Live WebSocket, session, audio
â”œâ”€â”€ Vision/               # Camera capture, frame throttling, QR detection
â”œâ”€â”€ Modes/                # Mode protocol, router, and built-in modes
â”œâ”€â”€ OpenClaw/             # Gateway bridge and tool call routing
â”œâ”€â”€ UI/                   # SwiftUI views
â””â”€â”€ docs/                 # Documentation
```

## Roadmap

| Phase | Focus | Timeline |
|-------|-------|----------|
| 1 | Foundation â€” app shell, camera, audio capture | Week 1-2 |
| 2 | Gemini Integration â€” WebSocket, streaming, tool calls | Week 3-4 |
| 3 | OpenClaw Bridge â€” gateway discovery, skill invocation | Week 5 |
| 4 | Mode System â€” all built-in modes, voice switching | Week 6-7 |
| 5 | Polish & Glasses â€” DAT SDK, glasses UI, optimization | Week 8-10 |
| 6 | Advanced Features â€” navigator, custom modes, widgets | Ongoing |

See [SPEC.md](SPEC.md) for full specification.

## Acknowledgments

- **[VisionClaw](https://github.com/sseanliu/VisionClaw)** by Sean Liu â€” the inspiration and proof-of-concept for Gemini Live + OpenClaw on smart glasses
- **[Gemini Multimodal Live API](https://ai.google.dev/gemini-api/docs/multimodal-live)** â€” Google's real-time multimodal streaming API
- **[OpenClaw](https://openclaw.app)** â€” AI agent gateway powering the 56+ skill integrations
- **[Meta DAT SDK](https://developers.meta.com/horizon/documentation/dat/dat-overview)** â€” Direct Audio Transfer SDK for Ray-Ban Meta glasses

## License

[MIT](LICENSE) Â© 2026 Mike Darlington
